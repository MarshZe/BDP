{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### １、从 Java 集合构造成 RDD\n",
    "List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);　\n",
    "\n",
    "JavaRDD<Integer> rdd = jsc.parallelize(data, 3)\n",
    "３是RDD分区个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 从文件 / 目录构造 RDD\n",
    "jsc.textFile(“/data”, 1)\n",
    "jsc.sequenceFile(“/data”, 1)\n",
    "jsc.wholeTextFiles(“/data”, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 初始化Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### map()操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return a new distributed dataset formed by passing each element of the source through a function func　接受一个函数，把这个函数作用于RDD中的每一个元素，将函数的返回结果作为结果RDD中对应元素的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 实例：求RDD中所有数的平方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x : x * x).collect()\n",
    "for num in squared:\n",
    "    print  (\"%i \" % num)\n",
    "sorted(squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 注意：map()的返回值类型不需要和输入类型一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)　接受一个函数分别应用到输入RDD的每一个元素上，不过返回的不是一个元素，而是一个返回值序列的迭代器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 实例：把输入的字符串切分为单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.flatMap(lambda line : line.split(\" \"))\n",
    "words.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "word = lines.map(lambda line : line.split(\" \"))\n",
    "word.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flatMap相当于把返回的迭代器“拍扁”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glom()操作\n",
    "将RDD中每个partition中元素转换为数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6, 7]]\n",
      "-------------\n",
      "ParallelCollectionRDD[6] at parallelize at PythonRDD.scala:475\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "result = nums.glom().collect()\n",
    "print(result)\n",
    "print(\"-------------\")\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new dataset formed by selecting those elements of the source on which func returns true.接受一个函数，并将RDD中满足该函数的元素放入新的RDD中返回\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "even = nums.filter(lambda x : x % 2 == 0)\n",
    "print(even.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.该函数与map函数类似，只不过映射函数的参数由RDD中的每一个元素变为RDD中的每一个分区的迭代器。如果在映射过程中需要频繁创建额外的对象，使用mapPartitions要比map更高效。比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都要创建一个connection,这样开销很大，如果使用mapPartions那么只需要为每一个分区创建一个connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "[3, 7, 18]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7], 3)\n",
    "print(nums.collect())\n",
    "def f(iterator):\n",
    "    while(iterator.hasNext)\n",
    "numsPartitions = nums.mapPartitions(f).collect()\n",
    "print(numsPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitionsWithIndex()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to mapPartitions, but also provides func with an integer value representing the index of the partition\n",
    "so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.同mapPartitions，\n",
    "不过提供了两个参数，第一个参数是分区的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7], 3)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "nums.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed\n",
    "对RDD采样，以及是否替换。\n",
    ":param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
    ":param fraction: expected size of the sample as a fraction of this RDD's size\n",
    "            without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
    "            with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    ":param seed: seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 5, 5, 6, 8, 10]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9,0,10])\n",
    "nums.sample(True, 0.6, 100).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new dataset that contains the union of the elements in the source dataset and the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 10, 11, 12]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num1 = sc.parallelize([1,2,3,4,5,6,7,8,9,0,10])\n",
    "num2 = sc.parallelize([11, 12])\n",
    "num1.union(num2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new RDD that contains the intersection of elements in the source dataset and the argument\n",
    "返回一个新的RDD，其中包含源数据集中元素和参数的交集；操作的RDD是相同的数据类型。只返回两个RDD中都有的元素。在运行时也会去掉所有重复的元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new dataset that contains the distinct elements of the source dataset.生成一个只包含不同元素的新RDD.distinct()操作的开销很大，因为它需要将所有的数据通过网络进行混洗，以确保每个元素只有一份。\n",
    "操作的RDD是相同的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 10]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([1, 10, 2, 3, 4, 5, 5, 3, 2])\n",
    "rdd3.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
    "Note: If you are grouping in order to perform an aggregation (such as a sum or average) \n",
    "over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
    "Note: By default, the level of parallelism in the output depends on the number of \n",
    "partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 1]), ('b', [1])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd4.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on a dataset of (K, V) pairs, \n",
    "returns a dataset of (K, V) pairs where the values \n",
    "for each key are aggregated using the given reduce function func, \n",
    "which must be of type (V,V) => V. Like in groupByKey, the number of \n",
    "reduce tasks is configurable through an optional second argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd5 = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd5.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. 聚合操作,需要传入排序函数和聚合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd６ = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)],3)\n",
    "def f(x, y) : return max(x,y)\n",
    "def c(x,y) : return x+y\n",
    "rdd6.aggregateByKey(0, f, c).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (4, 1), (1, 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd７ = sc.parallelize([(1, 1), (5, 1), (4, 1)],3)\n",
    "rdd7.sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, (1, 1)), (1, (1, 1)), (4, (1, 1))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd８ = sc.parallelize([(1, 1), (5, 1), (4, 1)],3)\n",
    "rdd９ = sc.parallelize([(1, 1), (5, 1), (4, 1)],2)\n",
    "rdd8.join(rdd9).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cogroup()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ([1], [2])), ('b', ([4], []))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartesian()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).\n",
    "求两个数据集的笛卡尔积。求大规模的笛卡尔积开销巨大。比如计算用户对各种产品的预期兴趣程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 2), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2])\n",
    "sorted(rdd.cartesian(rdd).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipe()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings. \n",
    "通过管道的方式对RDD的每个分区使用shell命令进行操作，返回相应的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '', '3']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['1', '2', '', '3'],3).pipe('cat').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset将RDD中的partition个数合并为numPartitions个.numPartitions必须比原始的分区数要小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3, 4, 5]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. 将RDD中的\n",
    "partition个数均匀合并为\n",
    "numPartitions个；非常消耗资源需要shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], []]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartitionAndSortWithinPartitions()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery.\n",
    "如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtract()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将原RDD里和参数RDD里相同的元素去掉Return each value in C{self} that is not contained in C{other}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glom()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将RDD中每个partition中元素转换为数组\n",
    " Return an RDD created by coalescing all elements within each partition\n",
    "        into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "sorted(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zips this RDD with another one, returning key-value pairs with the\n",
    "        first element in each RDD second element in each RDD, etc. Assumes\n",
    "        that the two RDDs have the same number of partitions and the same\n",
    "        number of elements in each partition (e.g. one was made through\n",
    "        a map on the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ａction操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reduce 它接受一个函数作为参数，这个函数要操作两个相同元素类型的RDD数据并返回一个同样类型的新元素。按照函数f对RDD中元素进行规约"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "sum = nums.reduce(lambda x , y : x + y)\n",
    "print (sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 需要我们提供需要返回的类型的初始值，然后再通过一个函数把RDD中的元素合并起来放入累加器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 将RDD转化为数组(Driver端)，需要慎用，如果数据量大会把所有数据发送给driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(nums.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 统计RDD中元素个数,并返回Long类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(nums.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 返回RDD中前k个元素,并保存成数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(nums.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对RDD中每个元素,调用函数f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the first element of the dataset (similar to take(1)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize(range(0,5))\n",
    "x.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed该方法仅在预期结果数组很小的情况下使用，因为所有数据都被加载到driver的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 9, 3, 4, 6]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(0, 10))\n",
    "print(rdd.collect())\n",
    "rdd.takeSample(True, 20, 1)\n",
    "rdd.takeSample(False, 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeOrdered()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the first n elements of the RDD using either their natural order or a custom comparator\n",
    "该方法仅在预期结果数组很小的情况下使用，因为所有数据都被加载到driver的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 7, 6, 5, 4]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsSequenceFile()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsObjectFile()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 2})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "x.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.\n",
    "Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
